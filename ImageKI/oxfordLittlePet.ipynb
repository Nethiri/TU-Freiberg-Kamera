{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\r\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\r\n",
    "\r\n",
    "import tensorflow_datasets as tfds\r\n",
    "\r\n",
    "from IPython.display import clear_output\r\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <PrefetchDataset shapes: {file_name: (), image: (None, None, 3), label: (), segmentation_mask: (None, None, 1), species: ()}, types: {file_name: tf.string, image: tf.uint8, label: tf.int64, segmentation_mask: tf.uint8, species: tf.int64}>, 'test': <PrefetchDataset shapes: {file_name: (), image: (None, None, 3), label: (), segmentation_mask: (None, None, 1), species: ()}, types: {file_name: tf.string, image: tf.uint8, label: tf.int64, segmentation_mask: tf.uint8, species: tf.int64}>}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='oxford_iiit_pet',\n",
      "    full_name='oxford_iiit_pet/3.2.0',\n",
      "    description=\"\"\"\n",
      "    The Oxford-IIIT pet dataset is a 37 category pet image dataset with roughly 200\n",
      "    images for each class. The images have large variations in scale, pose and\n",
      "    lighting. All images have an associated ground truth annotation of breed.\n",
      "    \"\"\",\n",
      "    homepage='http://www.robots.ox.ac.uk/~vgg/data/pets/',\n",
      "    data_path='C:\\\\Users\\\\User\\\\tensorflow_datasets\\\\oxford_iiit_pet\\\\3.2.0',\n",
      "    download_size=773.52 MiB,\n",
      "    dataset_size=774.69 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'file_name': Text(shape=(), dtype=tf.string),\n",
      "        'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=37),\n",
      "        'segmentation_mask': Image(shape=(None, None, 1), dtype=tf.uint8),\n",
      "        'species': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=3669, num_shards=4>,\n",
      "        'train': <SplitInfo num_examples=3680, num_shards=4>,\n",
      "    },\n",
      "    citation=\"\"\"@InProceedings{parkhi12a,\n",
      "      author       = \"Parkhi, O. M. and Vedaldi, A. and Zisserman, A. and Jawahar, C.~V.\",\n",
      "      title        = \"Cats and Dogs\",\n",
      "      booktitle    = \"IEEE Conference on Computer Vision and Pattern Recognition\",\n",
      "      year         = \"2012\",\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input_image, input_mask):\r\n",
    "  input_image = tf.cast(input_image, tf.float32) / 255.0\r\n",
    "  input_mask -= 1\r\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\r\n",
    "def load_image_train(datapoint):\r\n",
    "  input_image = tf.image.resize(datapoint['image'], (128, 128))\r\n",
    "  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\r\n",
    "\r\n",
    "  if tf.random.uniform(()) > 0.5:\r\n",
    "    input_image = tf.image.flip_left_right(input_image)\r\n",
    "    input_mask = tf.image.flip_left_right(input_mask)\r\n",
    "\r\n",
    "  input_image, input_mask = normalize(input_image, input_mask)\r\n",
    "\r\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_test(datapoint):\r\n",
    "  input_image = tf.image.resize(datapoint['image'], (128, 128))\r\n",
    "  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\r\n",
    "\r\n",
    "  input_image, input_mask = normalize(input_image, input_mask)\r\n",
    "\r\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LENGTH = info.splits['train'].num_examples\r\n",
    "BATCH_SIZE = 64\r\n",
    "BUFFER_SIZE = 1000\r\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']#.map(load_image_train, num_parallel_calls=tf.data.AUTOTUNE)\r\n",
    "test = dataset['test'].map(load_image_test)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RepeatDataset shapes: {file_name: (None,), image: (None, None, None, 3), label: (None,), segmentation_mask: (None, None, None, 1), species: (None,)}, types: {file_name: tf.string, image: tf.uint8, label: tf.int64, segmentation_mask: tf.uint8, species: tf.int64}>\n",
      "<PrefetchDataset shapes: {file_name: (None,), image: (None, None, None, 3), label: (None,), segmentation_mask: (None, None, None, 1), species: (None,)}, types: {file_name: tf.string, image: tf.uint8, label: tf.int64, segmentation_mask: tf.uint8, species: tf.int64}>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\r\n",
    "print(train_dataset)\r\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\r\n",
    "print(train_dataset)\r\n",
    "test_dataset = test.batch(BATCH_SIZE)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n",
      "<PrefetchDataset shapes: {file_name: (), image: (None, None, 3), label: (), segmentation_mask: (None, None, 1), species: ()}, types: {file_name: tf.string, image: tf.uint8, label: tf.int64, segmentation_mask: tf.uint8, species: tf.int64}>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n",
      "<PrefetchDataset shapes: ((None, 128, 128, 3), (None, 128, 128, 1)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "print(type(train))\r\n",
    "print(train)\r\n",
    "print(type(train_dataset))\r\n",
    "print(train_dataset)\r\n",
    "print(len(train_dataset))\r\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c8b08d2241c8157cc559a694afc7fe14d3ec73f4c204ededcfdcbff38dcf9d20"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}